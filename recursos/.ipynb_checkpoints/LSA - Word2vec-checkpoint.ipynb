{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd   \n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.matutils import cossim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from sklearn import manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Nunca pertenecería a un club que admitiera como socio a 1 persona como yo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nunca pertenecería a un club que admitiera como socio a 1 persona como yo'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/home/gastonb/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/home/gastonb/anaconda3/nltk_data'\n    - '/home/gastonb/anaconda3/share/nltk_data'\n    - '/home/gastonb/anaconda3/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-88e26567a79e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Tokenizar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/home/gastonb/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/home/gastonb/anaconda3/nltk_data'\n    - '/home/gastonb/anaconda3/share/nltk_data'\n    - '/home/gastonb/anaconda3/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Tokenizar\n",
    "tokens = word_tokenize(string)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# largo de la lista\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muestro el 4to elemento de la lista\n",
    "tokens[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muestro del 4to elemento de la lista en adelante\n",
    "tokens[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muestro hasta el 4to elemento de la lista\n",
    "tokens[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paso a uppercase e imprimo\n",
    "for s in tokens:\n",
    "    print( s.upper() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# si el token tiene una \"a\", lo paso a uppercase e imprimo\n",
    "for s in tokens:\n",
    "    if \"a\" in s:\n",
    "        print( s.upper() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No solo imprimo, sino que guardo las palabras en una lista\n",
    "tokens_filtrados=[]\n",
    "for s in tokens:\n",
    "    if \"a\" in s:\n",
    "        tokens_filtrados.append(s.upper())\n",
    "        print( tokens_filtrados )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muestro la lista guardada\n",
    "tokens_filtrados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de limpieza  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# el .decode(\"utf-8\") le dice a python que este tecto tiene caracteres \"latinos\"\n",
    "string = \"Nunca pertenecería a un club que admitiera como socio a 1 persona como yo. No puedo decir que no estoy en desacuerdo contigo. Citadme diciendo que me han citado mal (Ni $10.5)\"\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remplazo los signos $\n",
    "string_step1 = string.replace(\"$\",\" signopesos \")  \n",
    "string_step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reemplazo numeros con \" num \" \n",
    "reg_num = re.compile(r\"\\d+[.,]?\\d*\") # Regular expression to search numbers\n",
    "string_step2 = reg_num.sub(\" NUM \",string_step1)  \n",
    "string_step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paso a lowercase\n",
    "string_step3 = string_step2.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(string_step3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiro los que no son alphabeticos \n",
    "tokens_cleaned = []\n",
    "for token in word_tokenize(string_step3):\n",
    "    if token.isalpha(): # acá resulta relevante si el texto esta en utf-8\n",
    "        tokens_cleaned.append(token)\n",
    "\n",
    "tokens_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separacion por oración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Nunca pertenecería a un club que admitiera como socio a 1 persona como yo. No puedo decir que no estoy en desacuerdo contigo. Citadme diciendo que me han citado mal (Ni $10.5)\"\n",
    "# remplazo los signos $\n",
    "string_step1 = string.replace(\"$\",\" signopesos \") \n",
    "# reemplazo numeros con \" num \" y  paso a lowercase \n",
    "string_step3 = reg_num.sub(\" NUM \",string_step1).lower()  \n",
    "string_step4 = sent_tokenize(string_step3)\n",
    "\n",
    "# tiro los tokens no alphabeticos \n",
    "string_cleaned = []\n",
    "for sent in string_step4:\n",
    "    tokens = []\n",
    "    for token in word_tokenize(sent):\n",
    "        if token.isalpha():\n",
    "            tokens.append(token)\n",
    "    string_cleaned.append(tokens)\n",
    "    \n",
    "string_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec tutorial\n",
    "## Corpus Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open(\"diario_09-11.txt\", \"r\").read() # read corpus\n",
    "corpus[:1000] # mostrar primeras 1000 palabras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize(corpus[:1000]) # armo una lista de oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ ir corriendo ##########\n",
    "# remplazo los signos $\n",
    "corpus = corpus.replace(\"$\",\" signopesos \") \n",
    "# reemplazo numeros con \" num \" y  paso a minuscula \n",
    "corpus = reg_num.sub(\" NUM \",corpus).lower()  \n",
    "corpus = sent_tokenize(corpus)\n",
    "# tiro los tokens no alphabeticos \n",
    "trainset = []\n",
    "for sent in tqdm(corpus):\n",
    "    tokens = []\n",
    "    for token in word_tokenize(sent):\n",
    "        if token.isalpha():\n",
    "            tokens.append(token)\n",
    "    trainset.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"el corpus tiene\",len(trainset), \"oraciones y\",sum([len(x) for x in trainset]),\"palabras\"   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtro oraciones cortas\n",
    "trainset2 = [] \n",
    "for sent in trainset:\n",
    "    if len(sent)>3:\n",
    "        trainset2.append(sent)\n",
    "\n",
    "print( \"el corpus tiene\",len(trainset2), \"oraciones y\",sum([len(x) for x in trainset2]),\"palabras\"  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"window\" es el tamaño de la ventana. windows = 10, usa 10 palabras a la izquierda y 10 palabras a la derecha\n",
    "# \"n_dim\" es la dimension (i.e. el largo) de los vectores de word2vec\n",
    "# \"workers\" es el numero de cores que usa en paralelo. Para aprobechar eso es necesario tener instalado Cython)\n",
    "# \"sample\": word2vec filtra palabras que aparecen una fraccion mayor que \"sample\"\n",
    "# \"min_count\": Word2vec filtra palabras con menos apariciones que  \"min_count\"\n",
    "# \"sg\": para correr el Skipgram model (sg = 1), para correr el CBOW (sg = 0)\n",
    "# para mas detalle ver: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "n_dim = 20\n",
    "w2v_model = Word2Vec(trainset2, workers=4,size=n_dim, min_count = 10, window = 10, sample = 1e-3,negative=10,sg=1)\n",
    "#w2v_model.save(\"word2vec_diario_20dim\")  # save model\n",
    "#w2v_model = Word2Vec.load(\"word2vec_tasa\")  # load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(\"word2vec_diario_20dim\")  # load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the output of a word2vec representation is a numpy array \n",
    "w2v_model[\"arte\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"mujer-cocina similarity:\",w2v_model.wv.n_similarity([\"mujer\"], [\"cocina\"]))\n",
    "print (\"hombre-cocina similarity:\",w2v_model.wv.n_similarity([\"hombre\"], [\"cocina\"]) )\n",
    "print (\"\\n\")\n",
    "print (\"mujer-esposa similarity:\",w2v_model.wv.n_similarity([\"mujer\"], [\"esposa\"]) )\n",
    "print (\"hombre-esposo similarity:\",w2v_model.wv.n_similarity([\"hombre\"], [\"esposo\"]) )\n",
    "print(\"\\n\")\n",
    "print (\"mujer-hijos similarity:\",w2v_model.wv.n_similarity([\"mujer\"], [\"hijos\"]) )\n",
    "print (\"hombre-hijos similarity:\",w2v_model.wv.n_similarity([\"hombre\"], [\"hijos\"]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.most_similar(positive=[\"biología\"], negative=[], topn=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.most_similar(positive=[\"computación\"], negative=[], topn=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word=\"crimen\"\n",
    "barrios = [\"belgrano\",\"caballito\",\"ortúzar\",\"palermo\",\"recoleta\",\"núñez\",\"lugano\",\"pompeya\",\"martelli\",\"flores\",\"barracas\",\"soldati\",\"cañitas\"]\n",
    "crimen = []\n",
    "for word in barrios:\n",
    "    crimen.append(w2v_model.wv.n_similarity([target_word], [word]))\n",
    "    \n",
    "pd.DataFrame(crimen,index = barrios,columns=[target_word]).sort_values(by=target_word).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_robos = [\"robos\",\"armas\",\"asesinato\",\"ladrones\",\"hurto\",\"asalto\"]\n",
    "p_ciencias = [\"biología\",\"química\",\"matemática\",\"filosofía\",\"psicología\",\"ciencia\",\"ingeniería\"]\n",
    "p_tiempo = [\"lluvioso\",\"soleado\",\"calor\",\"nublado\",\"nieve\",\"tormenta\"]\n",
    "p_paises = [\"suiza\",\"suecia\",\"francia\",\"holanda\",\"australia\",\"perú\",\"bolivia\",\"paraguay\",\"uruguay\",\"brasil\",\"colombia\"]\n",
    "colores = [\"black\"]*len(p_robos)+[\"blue\"]*len(p_ciencias)+[\"green\"]*len(p_tiempo)+[\"red\"]*len(p_paises) \n",
    "grupos = [p_robos,p_ciencias,p_tiempo,p_paises]\n",
    "palabras = p_robos + p_ciencias + p_tiempo + p_paises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armo una matriz de distancias\n",
    "distancias=np.zeros((len(palabras),len(palabras))) #matriz cuadrada\n",
    "for i,ti in enumerate(palabras):\n",
    "    for j,tj in enumerate(palabras):\n",
    "        distancias[i,j] = abs(1-w2v_model.wv.similarity(ti,tj))\n",
    "print (distancias.shape)\n",
    "distancias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduccion de la dimensionalidad y visualizacion \n",
    "from sklearn.manifold import TSNE \n",
    "tsne = TSNE(n_components=2,metric=\"precomputed\",learning_rate=1000, random_state=123)\n",
    "np.set_printoptions(suppress=True)\n",
    "plt.figure(figsize=(8,8))\n",
    "Y = tsne.fit_transform(distancias)\n",
    "plt.scatter(Y[:, 0], Y[:, 1],color=\"black\",s=3)\n",
    "for label, x, y, color in zip(palabras, Y[:, 0], Y[:, 1],colores):\n",
    "    plt.annotate(label, xy=(x, y), xytext=(0, 0),color=color, textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analisis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dictionary wich maps tokens with Ids\n",
    "dictionary = corpora.Dictionary(trainset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dictionary.iteritems())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter words with low frequency (less than 5) \n",
    "dictionary.filter_extremes(no_below=10, no_above=1, keep_n=100000)\n",
    "list(dictionary.iteritems())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('spanish')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords_id =np.array(dictionary.doc2bow(stopwords.words('spanish')))[:,0]\n",
    "dictionary.filter_tokens(bad_ids=stopwords_id, good_ids=None)\n",
    "dictionary.save(\"diarios_dictionary.dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(line) for line in trainset2]\n",
    "tfidf = models.TfidfModel(corpus)  # tf-idf  transformation\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 100\n",
    "lsa_tfidf = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics = n_topics)  # initialize an LSI transformation\n",
    "#lsa_tfidf.save(\"LSA_diarios.lsi\") # Save LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load LSA\n",
    "lsa_tfidf = models.LsiModel.load(\"LSA_diarios.lsi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset2[1111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[1111] # (id,frequency) of the tokens in document 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"id num ->\", dictionary.doc2bow([\"num\"]),\"(2 rep)\") # id -> token\n",
    "print (\"id noches ->\", dictionary.doc2bow([\"noches\"]),\"(1 rep)\") # id -> token\n",
    "print (\"id mismo ->\", dictionary.doc2bow([\"mismo\"]),\"(1 rep)\") # id -> token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [dictionary.doc2bow([\"num\"])[0][0],dictionary.doc2bow([\"noches\"])[0][0],dictionary.doc2bow([\"mismo\"])[0][0]]\n",
    "pd.Series(dict(corpus_tfidf[1111]))[ids]# (id,tf-idf weight) of the tokens in document 1111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorial representation of a token\n",
    "vect_arte = lsa_tfidf[dictionary.doc2bow([\"arte\"])] \n",
    "vect_arte[:10] # shows only the first 10 elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the components of topic 2\n",
    "lsa_tfidf.show_topic(19, topn=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armo una matriz de distancias\n",
    "distancias_lsa=np.zeros((len(palabras),len(palabras))) #matriz cuadrada\n",
    "for i,ti in enumerate(palabras):\n",
    "    for j,tj in enumerate(palabras):\n",
    "        distancias_lsa[i,j] = abs(1-cossim(lsa_tfidf[dictionary.doc2bow([ti])] ,lsa_tfidf[dictionary.doc2bow([tj])]))\n",
    "print( distancias_lsa.shape )\n",
    "distancias_lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduccion de la dimensionalidad y visualizacion \n",
    "from sklearn.manifold import TSNE \n",
    "tsne = TSNE(n_components=2,metric=\"precomputed\",learning_rate=1000, random_state=123)\n",
    "np.set_printoptions(suppress=True)\n",
    "plt.figure(figsize=(8,8))\n",
    "Y = tsne.fit_transform(distancias_lsa)\n",
    "plt.scatter(Y[:, 0], Y[:, 1],color=\"black\",s=3)\n",
    "for label, x, y, color in zip(palabras, Y[:, 0], Y[:, 1],colores):\n",
    "    plt.annotate(label, xy=(x, y), xytext=(0, 0),color=color, textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
