{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from ipynb.fs.defs.preprocess import lemmatizer, remove_stopwords, sentence_tokenizer\n",
    "from pandas import DataFrame\n",
    "from collections import defaultdict\n",
    "from nltk import word_tokenize\n",
    "import gensim\n",
    "import numpy as np\n",
    "import dill\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Text_preprocessor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, func=None):\n",
    "        self.clean_func = remove_stopwords\n",
    "        return None\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        print(\"fit Text_preprocessor\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        print(\"Start transform Text_preprocessor\")\n",
    "        print(type(X))\n",
    "        print(self.clean_func)\n",
    "        function = dill.loads(dill.dumps(lambda x: self.clean_func(x)))\n",
    "        X_cleaned = X.apply(function)\n",
    "        return X_cleaned\n",
    "\n",
    "# Ejemplo\n",
    "#preproc = Text_preprocessor(preprocess)\n",
    "#preproc.fit_transform(df.raw_text)\n",
    "#preproc.transform(df.raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Text_preprocessor_sentence_tokenize(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self,func=None):\n",
    "        self.clean_func = sentence_tokenizer\n",
    "        return None\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        print(\"fit Text_preprocessor_sentence_tokenizer\")\n",
    "        print(self.clean_func)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        print(\"Start transform Text_preprocessor_sentence_tokenizer\")\n",
    "        print(type(X))\n",
    "        print(self.clean_func)\n",
    "        function = dill.loads(dill.dumps(lambda x: self.clean_func(x)))\n",
    "        X_cleaned = X.apply(function)\n",
    "        return X_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Text_preprocessor_lemmatizer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self,func=None):\n",
    "        self.clean_func = lemmatizer\n",
    "        return None\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        print(\"fit Text_preprocessor_lemmatizer\")\n",
    "        print(self.clean_func)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        print(\"Start transform Text_preprocessor_lemmatizer\")\n",
    "        print(type(X))\n",
    "        print(self.clean_func)\n",
    "        function = dill.loads(dill.dumps(lambda x: self.clean_func(x)))\n",
    "        X_cleaned = X.apply(function)\n",
    "        return X_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Estimador que crea nuevos features a partir del texto\n",
    "# Como entrada necesita un diccionario con referencia a las funciones que procecen texto y devuelvan un valor por cada documento\n",
    "# A modo de ejemplo cree funcione anonimas lambda, pero se podria utilizar con funciones convencionales que devuelvan un valor\n",
    "# el x de las funciones lambda son los subtitulos de cada pelicula.\n",
    "\n",
    "func_dict = {\n",
    "            \"largo_char_count\" : dill.loads(dill.dumps(lambda x: len(x))),\n",
    "            \"cant_palabras\": dill.loads(dill.dumps(lambda x: len(x.split(' ')))),\n",
    "            \"letras_x_palabras\" : dill.loads(dill.dumps(lambda x: 1.0*len(x)/len(x.split(' ')))),\n",
    "            \"vocales_num\":  dill.loads(dill.dumps(lambda x: len([l for l in x if l in \"aeiou\"]))),\n",
    "            }\n",
    "\n",
    "class Text_extrafeatures(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, func_dict):\n",
    "        self.func_dict = func_dict\n",
    "        self.func_dict_keys = list(func_dict.keys())\n",
    "        return None\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def get_feature_names(self):\n",
    "        return self.func_dict_keys\n",
    "    def transform(self, X):\n",
    "        print(\"Start transform EXTRA FEATURES\")\n",
    "        f = dill.dumps(lambda x, k: self.func_dict[k](x))\n",
    "        function = dill.loads(f)\n",
    "        result = {}\n",
    "        for k in self.func_dict_keys:\n",
    "            result[k] = X.apply(function,args=(k,))\n",
    "        return DataFrame(result).values\n",
    "\n",
    "#Ejemplo\n",
    "#extra_feat = Text_extrafeatures(func_dict)\n",
    "#res = extra_feat.fit_transform(Series(texts))\n",
    "#DataFrame(res, columns= extra_feat.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#a version that uses tf-idf weighting scheme for good measure\n",
    "class TfidfEmbeddingVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, tfidfv):\n",
    "        self.word2vec = None\n",
    "        self.word2weight = None\n",
    "        self.dim = 0 #len(word2vec.get(list(w2v.keys())[0]))\n",
    "        self.tfidf = tfidfv\n",
    "        \n",
    "    def fitTfidfVectorizer(self, X):\n",
    "        self.tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(self.tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            dill.dumps(lambda: max_idf,\n",
    "            [(w, self.tfidf.idf_[i]) for w, i in self.tfidf.vocabulary_.items()]))\n",
    "    \n",
    "    def fitW2V(self,X):\n",
    "        sentences=[[word for word in word_tokenize(doc)]for doc in X]\n",
    "        self.model = gensim.models.Word2Vec(\n",
    "            sentences,\n",
    "            size=50,#100 por defecto\n",
    "            #window=10, #5 por defecto\n",
    "            min_count=1) #elimina todas las palabras que tengan menos de esta frecuencia, por defecto 5\n",
    "            #workers=10) #para procesamiento en paralelo\n",
    "        self.model.train(sentences, total_examples=len(sentences), epochs=10)\n",
    "        self.word2vec = dict(zip(self.model.wv.index2word, self.model.wv.vectors))\n",
    "        self.dim = len(self.word2vec.get(list(self.word2vec.keys())[0]))\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.fitTfidfVectorizer(X)\n",
    "        self.fitW2V(list(X))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(\"EMBEDDING\")\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[word] * self.word2weight[word]\n",
    "                         for word in doc if word in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for doc in X\n",
    "            ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CollocationGenerator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, min_count=5,threshold=0.5,scoring='npmi'):\n",
    "        self.gensim_model=None\n",
    "        self.min_count=min_count\n",
    "        self.threshold=threshold\n",
    "        self.scoring=scoring\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        sentences=[item for sublist in X for item in sublist]\n",
    "        collocations = Phrases(sentences=sentences, min_count=self.min_count,threshold=self.threshold,scoring=self.scoring)\n",
    "        self.gensim_model = Phraser(collocations)\n",
    "        return self\n",
    "           \n",
    "    def transform(self, X):\n",
    "        print(\"Start transform COLLOCATIONS\")\n",
    "        f = dill.dumps(lambda x: self.subtitle(self.gensim_model[x]))\n",
    "        function = dill.loads(f)\n",
    "        X_cleaned = X.apply(function)\n",
    "        return X_cleaned\n",
    "    \n",
    "    def flat_list(self, X):\n",
    "        return [item for sublist in X for item in sublist]\n",
    "    \n",
    "    def subtitle(self, X):\n",
    "        return ' '.join(self.flat_list(X))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
