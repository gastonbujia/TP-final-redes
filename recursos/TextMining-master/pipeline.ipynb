{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pip install ipynb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gensim.sklearn_api.phrases import PhrasesTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.pipeline_class import Text_preprocessor, Text_preprocessor_sentence_tokenize, Text_extrafeatures, func_dict, TfidfEmbeddingVectorizer, CollocationGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PRECONDICION, LOS DATOS SE OBTIENEN DE dataframe.pkl QUE TIENE LOS SUBTITULOS YA LIMPIOS \n",
    "#Creacion de Pipe de preprocesamiento\n",
    "def dataset_pipeline(LSA_K = 10,LDA_TOPICS = 10,max_df_tfidf=0.9, min_df_tfidf=0.1,\n",
    "                    max_df_cv=0.9 , min_df_cv=0.1):\n",
    "    #%%time\n",
    "    #LSA_K -- cantidad de \"topicos\" LSA\n",
    "    #LDA_TOPICS -- cantidad de topicos LDA\n",
    "\n",
    "    tf_idf_vect = TfidfVectorizer(max_df_tfidf, min_df_tfidf, stop_words=\"english\" )\n",
    "    tf_idf_transf = TfidfTransformer() #trasnforma el count_vectorizer en tfidf, unsando el transformador, me evito volver a calcularlo\n",
    "    count_vect = CountVectorizer(max_df_cv, min_df_cv, stop_words=\"english\")\n",
    "    new_features = Text_extrafeatures(func_dict) # func_dict es para alacenar funciones que crean nuevos features\n",
    "    SVD = TruncatedSVD(n_components=LSA_K, algorithm = 'arpack')\n",
    "    lda = LatentDirichletAllocation(n_components=LDA_TOPICS, max_iter=100, learning_method=\"online\", n_jobs=-1)\n",
    "    w2v = TfidfEmbeddingVectorizer(tf_idf_vect)\n",
    "    lemma = Text_preprocessor()\n",
    "    sent_token = Text_preprocessor_sentence_tokenize()\n",
    "    collocations = CollocationGenerator()\n",
    "    sw = Text_preprocessor_lemmatizer()\n",
    "    \n",
    "    text_preproc = Pipeline([(\"lemmatizer\",lemma), \n",
    "                             (\"sent_token\",sent_token),\n",
    "                             (\"collocations\",collocations),\n",
    "                             (\"sw\",sw)\n",
    "                            ])\n",
    "\n",
    "    lsa_tfidf_pipe = Pipeline([(\"tfidf_transf\", tf_idf_transf),\n",
    "                                (\"lsa\",SVD)\n",
    "                               ])\n",
    "\n",
    "    topic_pipe = Pipeline([\n",
    "                            (\"tf_vec\", count_vect),\n",
    "                            (\"topic_features\",FeatureUnion([\n",
    "                                                            (\"lda\",lda),\n",
    "                                                            (\"lsa_tfidf_pipe\",lsa_tfidf_pipe),\n",
    "                                                            ]))\n",
    "                        ])\n",
    "\n",
    "\n",
    "    new_features_pipe = Pipeline([\n",
    "                                    (\"new_features\",new_features),\n",
    "                                    (\"std_scaler\",StandardScaler())\n",
    "                               ])\n",
    "\n",
    "#Se comenta text preproc y w2v porque llevaban mucho tiempo de ejecuci√≥n\n",
    "    dataset_pipeline = Pipeline([#(\"text_preproc\", text_preproc),\n",
    "                                 (\"text_features\",FeatureUnion([\n",
    "                                                                (\"new_features\",new_features_pipe),\n",
    "                                                                (\"topics\",topic_pipe),\n",
    "                                                                #(\"embedding\",w2v)\n",
    "                                                                ]))\n",
    "\n",
    "\n",
    "                                ])\n",
    "\n",
    "    return dataset_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataset_result= dataset_pipeline().fit_transform(X_train)\n",
    "#feature_names = new_features.get_feature_names() +[\"LSA_%d\"%(k+1) for k in np.arange(LSA_K)] + [\"LDA_%d\"%(k+1) for k in np.arange(LDA_TOPICS)] + tf_idf_vect.get_feature_names()\n",
    "#dataset_result_df = DataFrame(dataset_result.todense(), columns=feature_names, index = X_train.index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
